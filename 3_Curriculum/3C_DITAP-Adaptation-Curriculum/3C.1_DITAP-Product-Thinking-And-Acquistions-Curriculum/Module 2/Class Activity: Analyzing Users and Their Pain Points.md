# Class Activity: Analyzing Users and Their PainPoints  
This two-part activity gives you the chance to apply what you’ve learned about discovery tools. To begin, you’ll analyze a fictional user journey to identify pain points and uncover opportunities for improvement. In Part Two, you’ll define success criteria that reflect real user outcomes based on that opportunity.

Together, these exercises will help you practice translating user insights into actionable guidance—just like a discovery team would do before shaping a procurement strategy.

## Activity Instructions:

**Scenario Refresher:**  
Citizens across the country want to register their emotional support animals (ESAs) and receive official recognition for their calming companions. However, the current ESA landscape is cluttered with unofficial websites, conflicting policies, and skeptical service industry personnel and landlords. The National Office of Things & Stuff (NOTS) has been tasked with conducting a 20-minute discovery sprint to explore what a centralized, trustworthy, and humane ESA registry might look like.

**Instructions:**

1. Review Chloe’s journey map  
2. Identify the top three pain points  
3. Propose one opportunity for improvement  
4. Frame a “how might we” question based on that opportunity

### Journey map analysis
**Fictional Journey Map:** Chloe, College Student with Anxiety and a Ferret ("Professor Nibbles")</br>
</br>

| Stage | Action | Thoughts | Feelings |
| ----- | ----- | ----- | ----- |
| 1\. Searches online for the ESA registry | Googles "register emotional support animal" | So many sketchy-looking sites... which one is real? | Confused, overwhelmed |
| 2\. Clicks the top search result | Lands on a flashy third-party site charging $199 | Wait, is this even legal? | Skeptical, unsure |
| 3\. Tries to find official .gov site | Checks .gov results but finds inconsistent info | There’s no central place? What a mess. | Frustrated, distrustful |
| 4\. Asks campus therapist for help | The therapist says there’s no federal registry | Why do so many sites say there is one? | Irritated, discouraged |
| 5\. Gives up and downloads a free ESA letter template | Prints it and uses it with the landlord and the airline | Hope no one checks too hard… | Nervous, unsupported |

**Critical Thinking Prompts:**
- Where in the journey does trust break down?  
- What’s the most emotionally frustrating moment?  
- Where is the user most likely to be confused or misinformed?  
- Which pain point has the greatest impact and is the most fixable?  
- Did your group agree on the same pain points?  -* What kind of user data would help you validate your assumptions?

## Class Activity: Defining the Success Criteria 
Now that you’ve identified a key opportunity to improve the user experience, it’s time to define what success would look like. In this activity, you’ll work with your team to develop clear, measurable success criteria that reflect user needs and outcomes—not just system delivery. This step helps connect discovery insights to performance measures that can guide procurement planning and delivery oversight.

**Activity instructions**:

1. Revisit the opportunity you identified in the journey map activity.
2. As a team, define **2–3 success criteria** for that opportunity. Your criteria should reflect:
   - What success looks like for the *user* (not the agency).
   - Outcomes that can be observed, measured, or validated.
3. Share your top success metric during the debrief.

**Examples of User-Centered Success Criteria:**
- 80% of users say they trust the ESA registration site is legitimate.
- Landlords report fewer disputes related to ESA documentation.
- Users complete the ESA process in under 10 minutes without dropping off.
- No complaints from airlines about forged ESA documents within 6 months.

**Group Discussion: Reflect on Your Success Criteria**

After defining your success criteria, take a few minutes to discuss the following questions with your group. Be prepared to share one key insight with the class.
- How did your team decide your criteria were measurable? What made them realistic to track or evaluate?  
- Which user-centered values did you prioritize—ease, trust, accessibility, or something else? Why?  
- What kind of feedback or data would help you refine or validate your success metrics?
